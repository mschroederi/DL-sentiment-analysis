{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyUPpi5-gtFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c91fae63-5ceb-4bb6-9f75-7ea14b321c79"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvI9A3HehBl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "214ea9ac-dcd6-42a8-90a5-9bf062a03b73"
      },
      "source": [
        "!ls\n",
        "%cd drive/My\\ Drive/Colab\\ Notebooks"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n",
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOgrP9NgkxlE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89de603f-0b50-4613-8b49-543f44110dd2"
      },
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(\"data/train.csv\") and os.path.exists(\"data/test.csv\"):\n",
        "  print(\"No need to download data - already present!\")\n",
        "else:\n",
        "  os.mkdir(\"data\") \n",
        "  print(\"Created a data folder. Please place the files train.csv and test.csv in there!\")\n",
        "  raise Exception(\"Could not find train and test files. Please make sure to place them within the data folder (My Drive/Colab Notebooks/data).\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No need to download data - already present!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbobfUtvmNIz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efae7177-813c-49b8-f8cb-494f0b961d07"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")\n",
        "    raise Exception(\"Please use the GPU runtime! (Runtime -> Change runtime type -> GPU) Then you need to rerun the cells above!\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuHwcuaEgBx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import io\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "\n",
        "class MovieSentimentDataset(Dataset):\n",
        "    \"\"\"Movie sentiment dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with sentiments.\n",
        "        \"\"\"\n",
        "        self.movie_sentiments = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.movie_sentiments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        review = self.movie_sentiments.iloc[idx, 0]\n",
        "        #review  = np.array([review])\n",
        "\n",
        "        sentiment = self.movie_sentiments.iloc[idx, 1]\n",
        "        #sentiment  = np.array([sentiment])\n",
        "\n",
        "        sample = {'review': review, 'sentiment': sentiment}\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class MovieSentimentDatasetBuilder:\n",
        "    def __init__(self, data: pd.DataFrame):\n",
        "        self.data = data\n",
        "        self.train_validation_split = None\n",
        "\n",
        "    @staticmethod\n",
        "    def from_csv(csv_file: str):\n",
        "        return MovieSentimentDatasetBuilder(data=pd.read_csv(csv_file))\n",
        "    \n",
        "    def with_train_validation_split(self, splits: (float, float)=[.8, .2]):\n",
        "        self.train_validation_split = splits\n",
        "        return self\n",
        "    \n",
        "    def build(self):\n",
        "        if self.train_validation_split is None:\n",
        "            return MovieSentimentDataset(data=self.data)\n",
        "        else:\n",
        "            msk = np.random.rand(len(self.data)) < self.train_validation_split[0]\n",
        "            train = self.data[msk]\n",
        "            validation = self.data[~msk]\n",
        "            return (MovieSentimentDataset(data=train), MovieSentimentDataset(data=validation))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xFcY4mafz5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "SYMBOLS_TO_REMOVE = [\".\", \"\\\"\", \"(\", \")\", \",\", \"?\", \"!\", \"'\", \";\", \"{\", \"}\", \"-\", \"*\", \"=\", \":\", \"\\x91\", \"\\x97\", \"<br />\", \"/\", \"<\", \">\"]\n",
        "\n",
        "class Preprocessor:\n",
        "    @staticmethod\n",
        "    def remove_symbols(review_texts: pd.Series) -> pd.Series:\n",
        "        def preprocess_text(text: str):\n",
        "            for symbol in SYMBOLS_TO_REMOVE:\n",
        "                text = text.replace(symbol, \" \")\n",
        "            text = \" \".join([w for w in text.split() if w])\n",
        "            return text.lower()\n",
        "\n",
        "        return review_texts.str.lower().apply(preprocess_text)\n",
        "    \n",
        "    @staticmethod\n",
        "    def remove_long_sequences(df: pd.DataFrame, max_len: int) -> pd.Series:\n",
        "        seq_lengths = df[\"review\"].apply(lambda text: len(text.split()))\n",
        "        return df[seq_lengths <= max_len]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdBo-NX0fYkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import Tensor\n",
        "from sklearn import preprocessing\n",
        "from typing import List, Set\n",
        "\n",
        "class SequenceTokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vocab = dict({\"EOS\": 0})\n",
        "        self.padding_size = -1\n",
        "\n",
        "    def fit(self, reviews: pd.Series):\n",
        "        for review in reviews:\n",
        "            words_in_review = review.split()\n",
        "            if len(words_in_review) > self.padding_size:\n",
        "                self.padding_size = len(words_in_review)\n",
        "            for word in words_in_review:\n",
        "                if word not in self.vocab:\n",
        "                    self.vocab[word] = len(self.vocab)\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        print(\"Build a vocabulary of size {}\".format(self.vocab_size))\n",
        "\n",
        "    def __transform_single_review(self, review: str):\n",
        "        tokenized = np.array([self.vocab[word] for word in review.split() if word in self.vocab])\n",
        "        if len(tokenized) < self.padding_size:\n",
        "            tokenized = np.append(np.array([0 for _ in range(self.padding_size - len(tokenized))]), tokenized)\n",
        "        return torch.from_numpy(tokenized).long()\n",
        "\n",
        "    def transform(self, reviews: pd.Series):\n",
        "        return reviews.apply(self.__transform_single_review)\n",
        "\n",
        "    def fit_transform(self, reviews: pd.Series) -> pd.Series:\n",
        "        self.fit(reviews)\n",
        "        return self.transform(reviews)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y53aBIyzbMsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size: int, padding_size: int, embedding_size: int, hidden_size: int):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.padding_size = padding_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embbedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, dropout=1)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 32), nn.ReLU(),\n",
        "            nn.Linear(32, 4), nn.ReLU(),\n",
        "            nn.Linear(4, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, reviews):\n",
        "        reviews_embedded = self.embbedding(reviews)\n",
        "        reviews_embedded_permuted = reviews_embedded.permute(1, 0, 2)\n",
        "        lstm_output, lstm_hidden = self.rnn(reviews_embedded_permuted)\n",
        "        return self.out(lstm_hidden[0].reshape(-1, self.hidden_size))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCLyiKrPxTLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "class EarlyStopping():\n",
        "    def __init__(self, patience: int=3, checkpoint_path: str='best_model.pt', verbose: bool=True):\n",
        "        super().__init__()\n",
        "        self.patience = patience\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "        self.verbose = verbose\n",
        "        self.best_validation_loss = math.inf\n",
        "        self.counter = 0\n",
        "\n",
        "    def track(self, epoch: int, model, validation_loss: int):\n",
        "        if validation_loss < self.best_validation_loss:\n",
        "            if self.verbose:\n",
        "                print('Validation loss decreased from {:.4f} to {:.4f} in epoch {}.  Creating model checkpoint ...\\n'.format(self.best_validation_loss, validation_loss, epoch))\n",
        "            self.best_validation_loss = validation_loss\n",
        "            self.save_model(model)\n",
        "            self.counter = 0\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter > self.patience:\n",
        "                return True\n",
        "    \n",
        "    def save_model(self, model):\n",
        "        torch.save(model.state_dict(), self.checkpoint_path)\n",
        "\n",
        "    def get_best_version(self, model):\n",
        "        if self.best_validation_loss is math.inf:\n",
        "            raise Exception('Cannot bet best model. No model stored yet.')\n",
        "        model.load_state_dict(torch.load(self.checkpoint_path))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XD3WcwZfvPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "80fc390f-90ad-4249-b21d-4df5613237cd"
      },
      "source": [
        "import torch\n",
        "import sklearn\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
        "        enable_sampling = False\n",
        "        print(\"Running on the GPU\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        enable_sampling = True\n",
        "        print(\"Running on the CPU\")\n",
        "\n",
        "    dataset_train, dataset_validation = MovieSentimentDatasetBuilder\\\n",
        "        .from_csv(csv_file='data/train.csv')\\\n",
        "        .with_train_validation_split(splits=[.8, .2])\\\n",
        "        .build()\n",
        "    dataset_test = MovieSentimentDatasetBuilder\\\n",
        "        .from_csv(csv_file='data/test.csv')\\\n",
        "        .build()\n",
        "    \n",
        "    # Restrict the number of reviews if running on the CPU\n",
        "    if enable_sampling:\n",
        "        dataset_train.movie_sentiments = dataset_train.movie_sentiments.sample(100)\n",
        "        dataset_validation.movie_sentiments = dataset_validation.movie_sentiments.sample(20)\n",
        "\n",
        "    # Preprocess reviews\n",
        "    def execute_preprocessing_pipeline(dataset: MovieSentimentDataset, tokenizer=None):\n",
        "        reviews = dataset.movie_sentiments[\"review\"]\n",
        "        dataset.movie_sentiments[\"review\"] = Preprocessor.remove_symbols(dataset.movie_sentiments[\"review\"])\n",
        "        dataset.movie_sentiments = Preprocessor.remove_long_sequences(dataset.movie_sentiments, max_len=1000)\n",
        "\n",
        "        if tokenizer is None:\n",
        "            tokenizer = SequenceTokenizer()\n",
        "            tokenizer.fit(dataset.movie_sentiments[\"review\"])\n",
        "        dataset.movie_sentiments[\"review\"] = tokenizer.transform(dataset.movie_sentiments[\"review\"])\n",
        "        return tokenizer\n",
        "    \n",
        "    tokenizer = execute_preprocessing_pipeline(dataset_train)\n",
        "    vocab_size, padding_size = tokenizer.vocab_size, tokenizer.padding_size\n",
        "    execute_preprocessing_pipeline(dataset_validation, tokenizer=tokenizer)\n",
        "    execute_preprocessing_pipeline(dataset_test, tokenizer=tokenizer)\n",
        "\n",
        "    # Create DataLoader\n",
        "    dataloader_train = DataLoader(dataset_train, batch_size=256, shuffle=True, num_workers=1)\n",
        "    dataloader_validation = DataLoader(dataset_validation, batch_size=256, shuffle=False, num_workers=1)\n",
        "    dataloader_test = DataLoader(dataset_test, batch_size=256, shuffle=False, num_workers=1)\n",
        "\n",
        "    # Set up a bag of words model and training\n",
        "    embedding_size = 200\n",
        "    model = LSTMClassifier(vocab_size=vocab_size, padding_size=padding_size, embedding_size=embedding_size, hidden_size=128).to(device)\n",
        "    loss = nn.BCELoss()\n",
        "    num_epochs = 50\n",
        "    lr = 1e-2\n",
        "    trainer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
        "    early_stopping = EarlyStopping(patience=5)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss_epoch, train_acc, n = 0.0, 0, 0\n",
        "        model.train()\n",
        "        for i_batch, sample_batched in enumerate(dataloader_train):\n",
        "            y = sample_batched[\"sentiment\"].type(torch.FloatTensor).to(device).reshape(-1, 1)\n",
        "            y_hat = model(sample_batched[\"review\"].to(device).reshape(-1, padding_size))\n",
        "            \n",
        "            l = loss(y_hat, y)\n",
        "            train_loss_epoch += l.item()\n",
        "            trainer.zero_grad()\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_acc += (y == (y_hat > .5).type(torch.FloatTensor).to(device)).sum().item()\n",
        "            n += len(y)\n",
        "        \n",
        "        train_loss_epoch /= n\n",
        "        train_acc /= n\n",
        "        \n",
        "        validation_loss, validation_acc, n = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for i_batch, sample_batched in enumerate(dataloader_validation):\n",
        "                y = sample_batched[\"sentiment\"].type(torch.FloatTensor).to(device).reshape(-1, 1)\n",
        "                y_hat = model(sample_batched[\"review\"].to(device).reshape(-1, padding_size))\n",
        "                l = loss(y_hat, y)\n",
        "                validation_loss += l.item()\n",
        "                validation_acc += (y == (y_hat > .5).type(torch.FloatTensor).to(device)).sum().item()\n",
        "                n += len(y)\n",
        "            \n",
        "        validation_loss /= n\n",
        "        validation_acc /= n\n",
        "        print(\"Epoch: {}, Train Loss: {}, Train acc: {}, Validation Loss: {}, Validation acc: {}\".format(epoch+1, train_loss_epoch, train_acc, validation_loss, validation_acc))\n",
        "        \n",
        "        perform_early_stop = early_stopping.track(epoch=epoch, model=model, validation_loss=validation_loss)\n",
        "        if perform_early_stop:\n",
        "            print(\"Stopping early as no improvement was reached for {} epochs\".format(early_stopping.patience))\n",
        "            early_stopping.get_best_version(model)\n",
        "            break\n",
        "\n",
        "    test_loss, test_acc, n = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sample_batched in enumerate(dataloader_test):\n",
        "            y = sample_batched[\"sentiment\"].type(torch.FloatTensor).to(device).reshape(-1, 1)\n",
        "            y_hat = model(sample_batched[\"review\"].to(device).reshape(-1, padding_size))\n",
        "            l = loss(y_hat, y)\n",
        "            test_loss += l.item()\n",
        "            test_acc += (y == (y_hat > .5).type(torch.FloatTensor).to(device)).sum().item()\n",
        "            n += len(y)\n",
        "        \n",
        "    test_loss /= n\n",
        "    test_acc /= n\n",
        "    print(\"Test Loss: {}, Test acc: {}\".format(test_loss, test_acc))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "Build a vocabulary of size 68917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train Loss: 0.0025771741794092355, Train acc: 0.5996989463120923, Validation Loss: 0.002494532976090679, Validation acc: 0.6514875531268974\n",
            "Validation loss decreased from inf to 0.0025 in epoch 0.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 2, Train Loss: 0.0023456868459038323, Train acc: 0.6695935775213246, Validation Loss: 0.0022833621217229305, Validation acc: 0.7136207245496863\n",
            "Validation loss decreased from 0.0025 to 0.0023 in epoch 1.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 3, Train Loss: 0.0022884824290325817, Train acc: 0.6870546914199699, Validation Loss: 0.0024627982617100104, Validation acc: 0.6486541185994738\n",
            "Epoch: 4, Train Loss: 0.002341984090527517, Train acc: 0.6808329152032112, Validation Loss: 0.0022522088089734285, Validation acc: 0.7253592390204412\n",
            "Validation loss decreased from 0.0023 to 0.0023 in epoch 3.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 5, Train Loss: 0.0021015565253596057, Train acc: 0.7391871550426493, Validation Loss: 0.0017787437343906231, Validation acc: 0.8166363084395871\n",
            "Validation loss decreased from 0.0023 to 0.0018 in epoch 4.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 6, Train Loss: 0.002462995563747293, Train acc: 0.6322629202207727, Validation Loss: 0.002440434480770183, Validation acc: 0.6739526411657559\n",
            "Epoch: 7, Train Loss: 0.0022449059053933984, Train acc: 0.7095835423983944, Validation Loss: 0.0020339220032424632, Validation acc: 0.7609795587937664\n",
            "Epoch: 8, Train Loss: 0.001601477315003586, Train acc: 0.8191169091821375, Validation Loss: 0.0012516104313700432, Validation acc: 0.8664238008500303\n",
            "Validation loss decreased from 0.0018 to 0.0013 in epoch 7.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 9, Train Loss: 0.0012381079526804107, Train acc: 0.8681384846964375, Validation Loss: 0.001259617557218745, Validation acc: 0.8648046954057883\n",
            "Epoch: 10, Train Loss: 0.0011400145947245337, Train acc: 0.880080280983442, Validation Loss: 0.0011278291513166598, Validation acc: 0.8848411252782837\n",
            "Validation loss decreased from 0.0013 to 0.0011 in epoch 9.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 11, Train Loss: 0.001110157493933684, Train acc: 0.8828399397892625, Validation Loss: 0.0011253545573066059, Validation acc: 0.8858530661809351\n",
            "Validation loss decreased from 0.0011 to 0.0011 in epoch 10.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 12, Train Loss: 0.0010758228633670549, Train acc: 0.8872052182639237, Validation Loss: 0.0012492831601133426, Validation acc: 0.8652094717668488\n",
            "Epoch: 13, Train Loss: 0.0010386861363548283, Train acc: 0.8914701455092825, Validation Loss: 0.0010218744250449832, Validation acc: 0.890507994333131\n",
            "Validation loss decreased from 0.0011 to 0.0010 in epoch 12.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 14, Train Loss: 0.001015733415704843, Train acc: 0.8943803311590567, Validation Loss: 0.00104356613677418, Validation acc: 0.8961748633879781\n",
            "Epoch: 15, Train Loss: 0.000989233327462215, Train acc: 0.8979929754139488, Validation Loss: 0.0010529968739038439, Validation acc: 0.8915199352357822\n",
            "Epoch: 16, Train Loss: 0.0009496472375930044, Train acc: 0.9033617661816358, Validation Loss: 0.0010670307519295467, Validation acc: 0.8923294879579032\n",
            "Epoch: 17, Train Loss: 0.0009122120806217432, Train acc: 0.9059708981435023, Validation Loss: 0.0012654301334768866, Validation acc: 0.8767455980570734\n",
            "Epoch: 18, Train Loss: 0.0009073850045955426, Train acc: 0.9085298544907175, Validation Loss: 0.0011476613181584567, Validation acc: 0.8872697834446468\n",
            "Epoch: 19, Train Loss: 0.0008120528759217059, Train acc: 0.9204214751630707, Validation Loss: 0.0011033056489708596, Validation acc: 0.8892936652499493\n",
            "Stopping early as no improvement was reached for 5 epochs\n",
            "Test Loss: 0.0010608165787139511, Test acc: 0.8876661713321821\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}