{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUMDFbpdVpud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjd6s4_eMa2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p /content/drive/My\\ Drive/Colab\\ Notebooks/DL-sentiment-analysis/checkpoints"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvxz3tQ5t_SM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "db26fdfa-533a-40b1-dee2-daee33870273"
      },
      "source": [
        "!git clone https://github.com/mschroederi/DL-sentiment-analysis.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DL-sentiment-analysis'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 207 (delta 27), reused 49 (delta 7), pack-reused 131\u001b[K\n",
            "Receiving objects: 100% (207/207), 106.96 MiB | 12.27 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "Checking out files: 100% (43/43), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOTw_Q2oKM1a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "349922e3-c796-4ea5-81cf-95a7d33bc861"
      },
      "source": [
        "!ls\n",
        "%cd DL-sentiment-analysis/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DL-sentiment-analysis  drive  sample_data\n",
            "/content/DL-sentiment-analysis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfYSEp3HJwtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "cca786b3-87ee-4ee9-de01-f4b38b663e07"
      },
      "source": [
        "# train.py\n",
        "import torch\n",
        "\n",
        "from app.trainers.lstm_classifier_trainer import LSTMClassifierTrainer\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainer = LSTMClassifierTrainer(use_grid_search=False)\n",
        "    trainer.train(train_data_path='data/train.csv', \n",
        "                  model_checkpoint_path='/content/drive/My Drive/Colab Notebooks/DL-sentiment-analysis/checkpoints/model.pt', \n",
        "                  vocab_checkpoint_path='/content/drive/My Drive/Colab Notebooks/DL-sentiment-analysis/checkpoints/vocab.txt', \n",
        "                  num_epochs=50, patience=5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "Starting preprocessing pipeline...\n",
            "Completed preprocessing pipeline.\n",
            "Epoch: 1, Train Loss: 0.0026783743904529355, Train acc: 0.5566844381852932, Validation Loss: 0.0026480302970283707, Validation acc: 0.6163559831629585\n",
            "Validation loss decreased from inf to 0.0026 in epoch 0.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 2, Train Loss: 0.0024437821015735085, Train acc: 0.6476712604365757, Validation Loss: 0.00250805728921529, Validation acc: 0.6550410903988775\n",
            "Validation loss decreased from 0.0026 to 0.0025 in epoch 1.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 3, Train Loss: 0.0023547989953248962, Train acc: 0.6634644401971632, Validation Loss: 0.002398513300812156, Validation acc: 0.6760873922629785\n",
            "Validation loss decreased from 0.0025 to 0.0024 in epoch 2.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 4, Train Loss: 0.0021952182869953885, Train acc: 0.7196962076249874, Validation Loss: 0.002064695271539602, Validation acc: 0.7552615754660252\n",
            "Validation loss decreased from 0.0024 to 0.0021 in epoch 3.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 5, Train Loss: 0.0017993551539458592, Train acc: 0.7949904436173423, Validation Loss: 0.001645580540441229, Validation acc: 0.8214070956103428\n",
            "Validation loss decreased from 0.0021 to 0.0016 in epoch 4.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 6, Train Loss: 0.001526189660141015, Train acc: 0.829594608188311, Validation Loss: 0.0013548250032536944, Validation acc: 0.8606935257566647\n",
            "Validation loss decreased from 0.0016 to 0.0014 in epoch 5.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 7, Train Loss: 0.0011847682230526047, Train acc: 0.8771250377225631, Validation Loss: 0.0011875965171597195, Validation acc: 0.8763279214271397\n",
            "Validation loss decreased from 0.0014 to 0.0012 in epoch 6.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 8, Train Loss: 0.0010615028946851296, Train acc: 0.8913590182074238, Validation Loss: 0.0012574830826309932, Validation acc: 0.8723191020244538\n",
            "Epoch: 9, Train Loss: 0.0010000377962400035, Train acc: 0.8981490795694598, Validation Loss: 0.0011102125648984834, Validation acc: 0.8885548206053318\n",
            "Validation loss decreased from 0.0012 to 0.0011 in epoch 8.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 10, Train Loss: 0.0009509558460504018, Train acc: 0.903430238406599, Validation Loss: 0.001057716786538845, Validation acc: 0.897574664261375\n",
            "Validation loss decreased from 0.0011 to 0.0011 in epoch 9.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 11, Train Loss: 0.0009222600214462887, Train acc: 0.908258726486269, Validation Loss: 0.0011179155303662992, Validation acc: 0.8869512928442573\n",
            "Epoch: 12, Train Loss: 0.0008344221603473468, Train acc: 0.9167588773765215, Validation Loss: 0.0010951972958749606, Validation acc: 0.8911605532170775\n",
            "Epoch: 13, Train Loss: 0.0007987942038083889, Train acc: 0.9206820239412534, Validation Loss: 0.001107585486297164, Validation acc: 0.887352174784526\n",
            "Epoch: 14, Train Loss: 0.0007669509061264711, Train acc: 0.9247057640076451, Validation Loss: 0.0010439204557980439, Validation acc: 0.8965724594107035\n",
            "Validation loss decreased from 0.0011 to 0.0010 in epoch 13.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 15, Train Loss: 0.0007178929160627221, Train acc: 0.9310431546122121, Validation Loss: 0.0010427448500562322, Validation acc: 0.897574664261375\n",
            "Validation loss decreased from 0.0010 to 0.0010 in epoch 14.  Creating model checkpoint ...\n",
            "\n",
            "Epoch: 16, Train Loss: 0.0006911603003341587, Train acc: 0.93199879287798, Validation Loss: 0.0011337496314886984, Validation acc: 0.888755261575466\n",
            "Epoch: 17, Train Loss: 0.000650701979350401, Train acc: 0.9386882607383563, Validation Loss: 0.0012302095939267683, Validation acc: 0.8805371817999599\n",
            "Epoch: 18, Train Loss: 0.0005964334473686911, Train acc: 0.9429634845588974, Validation Loss: 0.0012316147101071203, Validation acc: 0.8903587893365403\n",
            "Epoch: 19, Train Loss: 0.0005851856134655777, Train acc: 0.9433658585655367, Validation Loss: 0.0011034508187777055, Validation acc: 0.8929645219482862\n",
            "Epoch: 20, Train Loss: 0.000563115502834416, Train acc: 0.9449753545920934, Validation Loss: 0.0011126580548252985, Validation acc: 0.899579073962718\n",
            "Epoch: 21, Train Loss: 0.0005324005577726367, Train acc: 0.9476410823860778, Validation Loss: 0.001195674021775939, Validation acc: 0.8821407095610343\n",
            "Stopping early as no improvement was reached for 5 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7lpPJY8LDAH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "82c77ea6-f998-4224-c264-7948487ef17c"
      },
      "source": [
        "# test.py\n",
        "from app.testers.lstm_classifier_tester import LSTMClassifierTester\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tester = LSTMClassifierTester()\n",
        "    tester.test(test_data_path='data/test.csv',\n",
        "                model_checkpoint_path='/content/drive/My Drive/Colab Notebooks/DL-sentiment-analysis/checkpoints/model.pt', \n",
        "                vocab_path='/content/drive/My Drive/Colab Notebooks/DL-sentiment-analysis/checkpoints/vocab.txt')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "Starting preprocessing pipeline...\n",
            "Completed preprocessing pipeline.\n",
            "Test Loss: 0.0012763278670538678, Test acc: 0.874251977991084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPOmeoBYRa6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c31da95a-7b9c-4f55-b5f6-617eb3ed4232"
      },
      "source": [
        "# write_review.py\n",
        "import argparse\n",
        "from tkinter import *\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from app.embeddings.sequence_tokenizer import SequenceTokenizer\n",
        "from app.models.lstm_classifier import LSTMClassifier\n",
        "from app.preprocessing.preprocessor import Preprocessor\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model: LSTMClassifier = torch.load('/content/drive/My Drive/Colab Notebooks/DL-sentiment-analysis/checkpoints/model.pt', map_location=device)\n",
        "padding_size = model.padding_size\n",
        "tokenizer = SequenceTokenizer.from_vocab_file('/content/drive/My Drive/Colab Notebooks/DL-sentiment-analysis/checkpoints/vocab.txt', padding_size)\n",
        "model.eval()\n",
        "print('Successfully loaded the model.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully loaded the model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGcW62tXR_hW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "9d3ec119-4eeb-43ec-dd02-f4668a7a1174"
      },
      "source": [
        "# write_review.py\n",
        "\n",
        "review = 'This was the best movie I\\'ve ever watched. I would recommend it to all my friends.'\n",
        "\n",
        "\n",
        "preprocessed = Preprocessor.preprocess_text(review)\n",
        "tokenized = tokenizer._SequenceTokenizer__transform_single_review(preprocessed)\n",
        "\n",
        "with torch.no_grad():\n",
        "    X = torch.tensor(tokenized).reshape(-1, padding_size).to(device)\n",
        "    y = model(X).reshape(-1, 1)\n",
        "    prob = y.flatten().item()\n",
        "percentage = np.round(prob * 100, 2)\n",
        "\n",
        "print(\"Positive Review: {}%\".format(percentage))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Review: 94.89%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxYjc9OdSdeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}